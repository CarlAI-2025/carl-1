package com.etl.noadk.agents;

import com.etl.noadk.domain.PipelineJob;
import lombok.extern.slf4j.Slf4j;

import java.util.*;

/**
 * Architect Agent: Pipeline generation.
 * Responsibilities:
 * - Generates runnable pipeline artifacts from mapping + transformation plan
 * - MVP: BigQuery SQL scripts + scheduled execution
 * - Scale (optional): Beam/Dataflow pipeline skeleton
 */
@Slf4j
public class ArchitectAgent implements ETLAgent {

    @Override
    public String getName() {
        return "Architect";
    }

    @Override
    public void execute(PipelineJob job) throws Exception {
        log.info("Architect: Generating pipeline artifacts");
        long startTime = System.currentTimeMillis();

        try {
            String sqlScript = generateBigQuerySQL(job);
            log.info("Architect: Generated BigQuery SQL script");

            // Add lineage
            PipelineJob.LineageEntry entry = new PipelineJob.LineageEntry("PIPELINE_GENERATION", "Architect");
            entry.setInputRecords(job.getStatistics().getTotalRecordsRead());
            entry.setOutputRecords(job.getStatistics().getTotalRecordsRead());
            job.getLineage().add(entry);

            job.setStatus(PipelineJob.JobStatus.VALIDATED);
            log.info("Architect: Pipeline ready for execution");

        } catch (Exception e) {
            log.error("Architect: Error during pipeline generation", e);
            PipelineJob.ErrorRecord error = new PipelineJob.ErrorRecord(
                    UUID.randomUUID().toString(),
                    "PIPELINE_GENERATION",
                    "GENERATION_ERROR",
                    e.getMessage()
            );
            job.getErrors().add(error);
            job.setStatus(PipelineJob.JobStatus.FAILED);
            throw e;
        }

        long duration = System.currentTimeMillis() - startTime;
        job.getLineage().get(job.getLineage().size() - 1).setDurationMs(duration);
    }

    private String generateBigQuerySQL(PipelineJob job) {
        StringBuilder sql = new StringBuilder();

        // CTEs for data cleaning
        sql.append("-- Auto-generated ETL Pipeline\n");
        sql.append("-- Generated by Architect Agent\n");
        sql.append("-- Dataset: ").append(job.getTargetDataset()).append("\n");
        sql.append("-- Table: ").append(job.getTargetTable()).append("\n\n");

        sql.append("WITH cleaned_source AS (\n");
        sql.append("  SELECT\n");
        sql.append("    TRIM(UPPER(id)) as security_id,\n");
        sql.append("    TRIM(name) as security_name,\n");
        sql.append("    CAST(amount AS NUMERIC) as transaction_amount,\n");
        sql.append("    PARSE_DATE('%Y-%m-%d', transaction_date) as transaction_date,\n");
        sql.append("    CURRENT_TIMESTAMP() as load_timestamp,\n");
        sql.append("    '").append(job.getJobId()).append("' as job_id\n");
        sql.append("  FROM `").append(job.getTargetDataset()).append(".staging_source`\n");
        sql.append("  WHERE TRIM(id) IS NOT NULL\n");
        sql.append("),\n");

        // Deduplication CTE
        sql.append("deduped_source AS (\n");
        sql.append("  SELECT DISTINCT ON (security_id)\n");
        sql.append("    *\n");
        sql.append("  FROM cleaned_source\n");
        sql.append("  ORDER BY security_id, transaction_date DESC\n");
        sql.append("),\n");

        // Enrichment CTE
        sql.append("enriched_data AS (\n");
        sql.append("  SELECT\n");
        sql.append("    ds.*,\n");
        sql.append("    mc.market_code,\n");
        sql.append("    mc.market_name\n");
        sql.append("  FROM deduped_source ds\n");
        sql.append("  LEFT JOIN `").append(job.getTargetDataset()).append(".market_codes_lookup` mc\n");
        sql.append("    ON ds.security_id = mc.security_id\n");
        sql.append(")\n");

        // Final insert
        sql.append("INSERT INTO `").append(job.getTargetDataset()).append(".").append(job.getTargetTable()).append("`\n");
        sql.append("SELECT * FROM enriched_data;\n\n");

        // Update job statistics table (for idempotent tracking)
        sql.append("-- Log job execution for idempotency\n");
        sql.append("INSERT INTO `").append(job.getTargetDataset()).append(".job_lineage`\n");
        sql.append("SELECT\n");
        sql.append("  '").append(job.getJobId()).append("' as job_id,\n");
        sql.append("  '").append(job.getTargetTable()).append("' as target_table,\n");
        sql.append("  CURRENT_TIMESTAMP() as execution_time,\n");
        sql.append("  COUNT(*) as records_loaded\n");
        sql.append("FROM `").append(job.getTargetDataset()).append(".").append(job.getTargetTable()).append("`\n");
        sql.append("WHERE job_id = '").append(job.getJobId()).append("'\n");
        sql.append("GROUP BY 1,2,3;\n");

        return sql.toString();
    }

    /**
     * Generate Dataflow template skeleton (for Platinum tier)
     */
    public String generateDataflowTemplate() {
        return """
                package com.etl.dataflow;
                
                import org.apache.beam.sdk.Pipeline;
                import org.apache.beam.sdk.io.gcp.bigquery.BigQueryIO;
                import org.apache.beam.sdk.io.gcp.bigquery.WriteResult;
                import org.apache.beam.sdk.io.TextIO;
                import org.apache.beam.sdk.options.PipelineOptions;
                import org.apache.beam.sdk.transforms.ParDo;
                
                public class ETLPipeline {
                    public static void main(String[] args) {
                        PipelineOptions options = PipelineOptionsFactory.fromArgs(args)
                            .withValidation()
                            .create();
                        
                        Pipeline p = Pipeline.create(options);
                        
                        // Read from GCS
                        p.apply("Read CSV", TextIO.read().from("gs://bucket/path/data.csv"))
                         .apply("Parse CSV", ParDo.of(new CSVParsingFn()))
                         .apply("Clean Data", ParDo.of(new DataCleaningFn()))
                         .apply("Validate", ParDo.of(new ValidationFn()))
                         .apply("Deduplicate", Deduplicate.byKey())
                         .apply("Enrich", ParDo.of(new EnrichmentFn()))
                         .apply("Load to BigQuery", BigQueryIO.writeTableRows()
                            .to("project:dataset.table")
                            .withCreateDisposition(BigQueryIO.Write.CreateDisposition.CREATE_IF_NEEDED)
                            .withWriteDisposition(BigQueryIO.Write.WriteDisposition.WRITE_APPEND));
                        
                        p.run().waitUntilFinish();
                    }
                }
                """;
    }
}

